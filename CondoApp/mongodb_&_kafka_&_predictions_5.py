# -*- coding: utf-8 -*-
"""mongodb_&_kafka_&_predictions_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1peXQ8Gu9XjzVRBF5XktvEJfR6vDLcqJQ

# Setup, installs, imports
"""
# UN-COMMENT WHEN USING COLAB
# from google.colab import drive
# drive.mount('/content/drive')

"""## Install dependencies"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install anvil-uplink
import anvil.server
# anvil.server.connect("server_5ARGRZHPIDNZYRBD57L6TB3G-UPZYF35YBJLIL4GJ")
anvil.server.connect("server_QSTGEBRVF7ELEXW35YFNHWS6-UPZYF35YBJLIL4GJ")

# ###########################
# !pip install pyspark
# ###########################
# !pip install kafka-python
# ###########################
# !pip install pymongo

"""### MongoDB, local instance"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %%bash
# sudo apt install -y mongodb >log
# service mongodb start

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# ps -ef | grep mongo

"""## Import dependencies"""

import json
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.ml.feature import *
from pyspark.ml import *
from pyspark.ml.evaluation import *
################
from kafka import KafkaProducer
from kafka.errors import KafkaError
from kafka import KafkaConsumer
import pprint
pp = pprint.PrettyPrinter(indent=4)
import json
###########
from pymongo import MongoClient

"""#### Create new db, new collection"""
# UN-COMMENT WHEN USING COLAB
# URI = "mongodb://localhost:27017"
# # get Mongo_Client
# client = MongoClient()
# # create db
# db = client["new_db"]
# # get all db names, verify new db created
# # client.list_database_names()
# # create collection
# db.create_collection("preds")
# # create a file-handle-like reference to collection
# colcn_preds = db.preds
# ########
# from pymongo.errors import ConnectionFailure
# client = MongoClient()
# try:
#     # The ping command is cheap and does not require auth.
#     client.admin.command('ping')
# except ConnectionFailure:
#     print("Server not available")

"""## Kafka setup
Download and setup Kafka and Zookeeper instances

following instances are setup locally:

    Kafka (Brokers: 127.0.0.1:9092)
    Zookeeper (Node: 127.0.0.1:2181)

"""
# UN-COMMENT WHEN USING COLAB
# !curl -sSOL https://dlcdn.apache.org/kafka/3.1.0/kafka_2.13-3.1.0.tgz
# !tar -xzf kafka_2.13-3.1.0.tgz

"""Using the default configurations (provided by Apache Kafka) for spinning up the instances."""
# UN-COMMENT WHEN USING COLAB
# !./kafka_2.13-3.1.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.1.0/config/zookeeper.properties
# !./kafka_2.13-3.1.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.1.0/config/server.properties
# !echo "Waiting for 10 secs until kafka and zookeeper services are up and running"
# !sleep 10

"""## Serializer function"""

def serializer(input_value):
    return json.dumps(input_value).encode('utf-8')

"""## De-Serializer function"""

def de_serializer(input_value):
    return json.loads(input_value)

"""### Kafka producer """

topic_input = 'user_input'  # kafka topic name
bootstrap_servers = ['localhost:9092']  # server and port info

# define/instantiate kafka producer
producer = KafkaProducer(bootstrap_servers= bootstrap_servers,
                             value_serializer= serializer,
                             acks= 1
                        )

if __name__ == "__main__":
    """## Spark setup"""

    spark = SparkSession.builder.appName('make_predictions_project').getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
    spark

    """# Import ML model"""

    # Commented out IPython magic to ensure Python compatibility.
    # %%capture
    file_path = "file:///D:/2_general_repo/HomeValuePrediction/model_saved"
    saved_model = PipelineModel.load(file_path)

    """### LOAD schema"""

    with open("df_etl_2_schema.json", 'r') as content_file:
      schema_json = content_file.read()

    new_schema = StructType.fromJson(json.loads(schema_json))
    print(new_schema)

    """# Functions for webapp
    
    ## Get User Input
    """

    @anvil.server.callable
    def predict_home_value(list_of_inputs):
      print("\n")
      print("recvd input list= ", list_of_inputs)

      # for input in list_of_inputs:
      #   print("input= ", input, ", type= ", type(input))

      user_input_floor_num = int(list_of_inputs[0])
      user_input_n_bathrooms = float(list_of_inputs[1])
      user_input_n_rooms = int(list_of_inputs[2])
      user_input_sq_mt_built = float(list_of_inputs[3])

      # print("After re_casting, user_input_floor_num= ", user_input_floor_num, ", type= ", type(user_input_floor_num))
      # print("After re_casting, user_input_n_bathrooms= ", user_input_n_bathrooms, ", type= ", type(user_input_n_bathrooms))
      # print("After re_casting, user_input_n_rooms= ", user_input_n_rooms, ", type= ", type(user_input_n_rooms))
      # print("After re_casting, user_input_sq_mt_built= ", user_input_sq_mt_built, ", type= ", type(user_input_sq_mt_built))

      floor_string_id_list = ['first_flr','second_flr','third_flr','fourth_flr','fifth_flr',
                              'sixth_flr','seventh_flr','eighth_flr','ninth_flr']

      user_input_floor_str = floor_string_id_list[user_input_floor_num-1]

      user_data = [ ( user_input_floor_str,
                    user_input_n_bathrooms,
                    user_input_n_rooms,
                    user_input_sq_mt_built
                    )
                  ]
      # call kafka_producer
      print("user_data sent to kafka_producer= ", user_data)
      # return_kafka_producer = kafka_producer(user_data)
      # print("return_kafka_producer= ", return_kafka_producer)

      topic_input = 'user_input'  # kafka topic name
      bootstrap_servers = ['localhost:9092']  # server and port info
      # define/instantiate kafka producer
      producer = KafkaProducer(bootstrap_servers= bootstrap_servers,
                                 value_serializer= serializer,
                                 acks= 1
                            )
      # send value to Kafka, but value is constructed as a dict instead of str
      producer.send(topic_input, {"user_input": user_data})
      # msg = "func call success, def kafka_producer"


      # call kafka_consumer
      # print("Calling kafka-consumer")
      # return_kafka_consumer = kafka_consumer()
      # print("return_kafka_consumer= ", return_kafka_consumer)
      # print("inside kafka Consumer")
      bootstrap_servers = ['localhost:9092']  # server and port info
      topic_input = 'user_input'  # kafka topic name
      # Kafka consumer definition
      consumer = KafkaConsumer(topic_input,
                             bootstrap_servers=bootstrap_servers,
                             auto_offset_reset= 'earliest', #'earliest', #'latest',
                             value_deserializer=de_serializer,
                             enable_auto_commit= True,
                             consumer_timeout_ms= 200
                             )
      # output what consumer received
      # NOTE THIS CONSUMER HAS TIME-OUT-EXIT. CONSUMER KEEPS LISTENING UNTIL TIME-OUT.
      for item in consumer:
          subscribed_obj_dict = item.value
          # value is a spark df containing user_input with appropriate schema

      keys_list =  list(subscribed_obj_dict.keys())
      last_index = len(keys_list)-1
      # print("subscribed_obj_dict; key= {}, value= {}".format(keys_list[last_index],
      #                                                       subscribed_obj_dict[keys_list[last_index]]))
      # define consumer_dump
      consumer_dump = subscribed_obj_dict[keys_list[last_index]][0]
      # print("consumer_dump= ", consumer_dump)
      # get components of consumer dump
      floor_string_id_list = ['first_flr','second_flr','third_flr','fourth_flr','fifth_flr',
                              'sixth_flr','seventh_flr','eighth_flr','ninth_flr']

      dump_floor="" # initialize empty string
      # print("initial, dump_floor= {}, type(dump_floor)= {}". format(dump_floor, type(dump_floor)))
      dump_floor= consumer_dump[0]
      # print("assigned, dump_floor= {}, type(dump_floor)= {}". format(dump_floor, type(dump_floor)))

      dump_n_bathrooms = consumer_dump[1]
      dump_n_rooms = consumer_dump[2]
      dump_sq_mt_built = consumer_dump[3]
      # construct user_data_2
      user_data_2 = [ ( dump_floor,
                   dump_n_bathrooms,
                   dump_n_rooms,
                   dump_sq_mt_built
                   )
                ]
      print("user_data_2 from kafka_consumer= ", user_data_2)

      # call func to reconstruct user_data_2
      # predicted_home_value = make_predictions(return_kafka_consumer)
      # print("Calling make_predictions(user_data_2) ")
      predicted_home_value = make_predictions(user_data_2)
      return "$"+ f"{predicted_home_value:,}"
    # predict_home_value()

    ###########################################################################################z
    @anvil.server.callable
    def make_predictions(user_data_2):
      print("inside make_predictions, user_data_2 recvd by make_predictions= ", user_data_2)
      df_user = spark.createDataFrame(user_data_2,schema=new_schema)
      # df_user.show()
      # predict on use input data
      df_preds = saved_model.transform(df_user)
      # drop un-necessary cols
      col_list_to_drop = ["floor_str_ix", "floor_encd_vec", "features"]
      df_preds = df_preds.drop(*col_list_to_drop)
      # select the predicted buy_price for user input
      pred_buy_price = df_preds.select('prediction').collect()[0][0]
      pred_buy_price_str = str(pred_buy_price)
      pred_buy_price_str =  pred_buy_price_str.split(".")
      pred_buy_price_str[1] = pred_buy_price_str[1][:2]
      pred_buy_price_str[1]
      # combine to one string
      y = pred_buy_price_str[0] + "." + pred_buy_price_str[1][0:2]
      y= float(y)
      y= (y**2)**0.5 # get absolute value.
      pred_buy_price = y
      print(f"predicted buy price= ${pred_buy_price:,}")
      return pred_buy_price

    ###########################################################################################z
    anvil.server.wait_forever() # the notebook is always available to the web app
